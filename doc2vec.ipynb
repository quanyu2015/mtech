{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocess\n",
    "\n",
    "data source: https://www.kaggle.com/crowdflower/twitter-airline-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 15)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "twt = pd.read_csv('Twitter_Airline_Sentiment.csv')\n",
    "twt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>pid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>id_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>positive</td>\n",
       "      <td>id_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>id_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>negative</td>\n",
       "      <td>id_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>negative</td>\n",
       "      <td>id_4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text airline_sentiment   pid\n",
       "0                @VirginAmerica What @dhepburn said.           neutral  id_0\n",
       "1  @VirginAmerica plus you've added commercials t...          positive  id_1\n",
       "2  @VirginAmerica I didn't today... Must mean I n...           neutral  id_2\n",
       "3  @VirginAmerica it's really aggressive to blast...          negative  id_3\n",
       "4  @VirginAmerica and it's a really big bad thing...          negative  id_4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paragraph Id\n",
    "twt['pid'] = 'id_' + twt.index.astype(str)\n",
    "twt[['text', 'airline_sentiment', 'pid']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"@AmericanAir Can't unload flight #3322 because jetway is broken.  #steps #planB? #waiting nearly an hour\",\n",
       " '@united This must be a drone “@united: @KeamBleam We understand your frustration. Our Bag team is working hard to get your bag(s) to you...\"',\n",
       " '@united employees almost seem happy when delivery terrible customer service.',\n",
       " '@united really, fill out a form about my flight experience? I sent an email to the 1K email address.',\n",
       " '@USAirways: I experienced what defines customer service on #FLT1999. A flight attendant willing to follow up with a passenger on bag charges',\n",
       " '@united you all do a wonderful job today. Got my wife, daughter, and myself from PGH to Orlando after out flight was delayed luggage and all',\n",
       " '@JetBlue does not fit in 140',\n",
       " '@united I just booked a flight for (2). When I view my reservation it has MI connected to First name. Is this a problem? can it be changed?',\n",
       " \"@United Airlines' CEO Jeff Smisek: Disloyal to Loyal Workers http://t.co/0cevY3P42b via @HuffPostBiz\",\n",
       " '@SouthwestAir update 35 minutes into this pilot has turned off a/c. No longer freezing, but we are enjoying waiting for reload of all bags']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twt['text'].sample(10).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    9178\n",
       "neutral     3099\n",
       "positive    2363\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twt['airline_sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html   \n",
    "https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7375      last flight was cancelled flightled then it w...\n",
       "10056     overloads small plane with extra baggage and ...\n",
       "4869      wifi is so slow it totally precludes working ...\n",
       "2315      u guys did it again changed ages and double b...\n",
       "14574     i dmed you my aa &amp; phone s &amp; you can'...\n",
       "6261      now it's delayed until 355 getting yelled at ...\n",
       "3582      then watched my connecting flight in den pull...\n",
       "13496     wasn't just a delay your counter wouldn't tak...\n",
       "7598      well the last update was in the right directi...\n",
       "3070      huge kudos to the fo of sunday's flt 1623 sjo...\n",
       "12155     this delayed bag was for my friend lisa pafe ...\n",
       "7896          fliers to gain access to wsj content   <url>\n",
       "7094                   x__x rt  our fleet's on fleek <url>\n",
       "8475      works with google chrome but not internet exp...\n",
       "10969     i rebooked myself  but cancelled flighting fl...\n",
       "14421     flt cancelled flighted rescheduled to bad tim...\n",
       "14588     if business class if full but 1st class empty...\n",
       "14442     how did my prime ticketed seat get switched o...\n",
       "6587      chances of flight leaving bna tomorrow at 620...\n",
       "1838      i assume that would be for the other 300 peop...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def cleanText(text):\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'http\\S+', r'<URL>', text)\n",
    "    text = re.sub('[\\#\\?\\,.\\!\\:\\-\\/\\\"]', '', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "abc = twt['text'].sample(20)\n",
    "abc.apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/test size:\n",
      "10248 4392\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "df = twt[['text', 'airline_sentiment', 'pid']].copy()\n",
    "df['text'] = df['text'].apply(cleanText)\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.3, random_state=42)\n",
    "print(\"train/test size:\")\n",
    "print(len(train), len(test))\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "\n",
    "train_tagged = train.apply(lambda r: TaggedDocument(words=tokenize_text(r['text']), tags=[r.pid]), axis=1)\n",
    "test_tagged = test.apply(lambda r: TaggedDocument(words=tokenize_text(r['text']), tags=[r.pid]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['exicted', 'to', 'be', 'flying', 'with', \"y'all\", 'soon'], tags=['id_10990'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged.values[130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['new', 'fas', 'from', 'dfw', 'to', 'clt', 'this', 'morning', 'did', 'great', 'job', 'well', 'done'], tags=['id_9997'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged.values[230]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10248/10248 [00:00<00:00, 2830823.72it/s]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from tqdm import tqdm\n",
    "\n",
    "# If dm=0, distributed bag of words (PV-DBOW) is used; if dm=1,‘distributed memory’ (PV-DM) is used.\n",
    "# vector_size=100, dimension of feature vectors\n",
    "# negative=5, specifies how many “noise words” should be drawn\n",
    "# hs=0, and negative is non-zero, negative sampling will be used. hs=1, hierarchical softmax will be used\n",
    "# min_count=2, ignores all words with total frequency lower than this.\n",
    "# workers=cores, use these many worker threads to train the model\n",
    "\n",
    "# build_vocab: Build vocabulary from a sequence of documents\n",
    "\n",
    "model_dbow = Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample=0, workers=cores)\n",
    "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10248/10248 [00:00<00:00, 2399420.98it/s]\n",
      "100%|██████████| 10248/10248 [00:00<00:00, 3077264.27it/s]\n",
      "100%|██████████| 10248/10248 [00:00<00:00, 3411367.25it/s]\n",
      "100%|██████████| 10248/10248 [00:00<00:00, 3242304.25it/s]\n",
      "100%|██████████| 10248/10248 [00:00<00:00, 3411367.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.86 s, sys: 292 ms, total: 2.15 s\n",
      "Wall time: 1.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn import utils\n",
    "\n",
    "for epoch in range(5):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dbow.alpha -= 0.01\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.7849168e-03, -3.5976903e-03,  2.6366175e-03,  2.0197590e-03,\n",
       "        4.9026613e-03, -4.0240064e-03,  4.1246125e-03, -1.3683724e-03,\n",
       "       -4.4880062e-03, -3.7045605e-03,  1.8961310e-03, -3.1398521e-03,\n",
       "        3.7168742e-03,  2.1768378e-03,  4.7869845e-03, -5.9758913e-04,\n",
       "        3.0824784e-03, -4.7984663e-03,  3.3126618e-03,  3.0297304e-03,\n",
       "        9.2157198e-04,  3.6062519e-03,  4.0312381e-03,  3.2966593e-03,\n",
       "       -1.8666973e-03, -2.6029947e-03, -1.3738588e-03,  3.0936552e-03,\n",
       "       -1.5778954e-03,  1.8296989e-03, -3.5384735e-03, -1.6363992e-03,\n",
       "        5.9242896e-04,  3.4157357e-03,  1.0186807e-03,  9.3581708e-04,\n",
       "        1.7825345e-03, -9.4159669e-04,  1.8399979e-03,  3.0631749e-03,\n",
       "       -6.4379035e-04,  2.7704926e-03, -4.2571332e-03,  7.0344936e-04,\n",
       "        1.5287144e-03, -3.4877951e-03, -1.8509105e-03, -4.7731753e-03,\n",
       "        3.7861951e-03, -5.2917912e-04, -2.4302639e-03,  3.6517745e-03,\n",
       "       -3.3845026e-03,  1.2606429e-04, -1.3725236e-03, -2.1271575e-03,\n",
       "       -9.9532818e-04, -7.1284629e-04,  8.9105970e-04, -3.9842495e-04,\n",
       "        3.0858559e-03, -3.4802584e-03,  3.7258090e-03,  2.6237641e-03,\n",
       "        2.8916472e-03,  1.7462283e-03,  3.9813514e-03, -2.2522672e-03,\n",
       "        2.9562290e-03,  1.5651113e-03,  2.6041651e-03, -4.9569446e-04,\n",
       "        3.2777460e-03, -1.5256749e-03, -5.2530654e-03, -2.6964848e-03,\n",
       "       -4.3311487e-03, -4.7899936e-03, -1.8573676e-03, -5.3333445e-04,\n",
       "        2.0650415e-04, -3.3509003e-03, -4.3928698e-03, -2.2189755e-03,\n",
       "       -4.9662068e-03, -3.2918428e-03,  8.7909418e-04,  4.8775696e-03,\n",
       "       -1.4388909e-04, -4.7213081e-03,  3.9077727e-03,  4.5609837e-03,\n",
       "        1.4311047e-03,  4.7442331e-03, -2.0918173e-03,  3.3478395e-03,\n",
       "       -4.8667202e-03, -3.0050550e-03,  9.0765767e-05, -1.3604943e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dbow.docvecs['id_9997']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('id_11351', 1.0), ('id_5507', 0.3673630654811859), ('id_13339', 0.3532582223415375), ('id_679', 0.34670791029930115), ('id_14201', 0.3388338088989258)]\n",
      "\n",
      "TaggedDocument(['marsha', 'at', 'myrtle', 'beach', 'is', 'the', 'greatest', 'she', 'deserves', 'all', 'the', 'respect', 'and', 'praise', 'there', 'is', 'thankyouforeverything'], ['id_11351'])\n",
      "\n",
      "TaggedDocument(['so', 'the', 'upcoming', 'rr', 'changesdeval', 'is', 'trying', 'to', 'tell', 'me', 'not', 'to', 'fly', 'southwest', 'anymore', 'because', 'am', 'loyal', 'so', 'far', 'get', 'it'], ['id_5507'])\n",
      "\n",
      "TaggedDocument(['thank', 'you'], ['id_13339'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_vector = model_dbow.docvecs['id_11351']\n",
    "sims = model_dbow.docvecs.most_similar([new_vector], topn=5)\n",
    "print(sims)\n",
    "print()\n",
    "\n",
    "for i in range(3):\n",
    "    idx = int(sims[i][0].split(\"_\")[1])\n",
    "    print(train_tagged[idx])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qy/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/qy/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.6723588342440802\n",
      "Testing F1 score: 0.6212614208818695\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def vec_for_learning(model, tagged_docs, data):\n",
    "    sents = tagged_docs.values\n",
    "    \n",
    "    # infer_vector: A document for which the vector representation will be inferred.\n",
    "    # Number of times to train the new document. \n",
    "    \n",
    "    regressors = [model.infer_vector(doc.words, epochs=20) for doc in sents]\n",
    "    targets = data['airline_sentiment'].tolist()\n",
    "    return targets, regressors\n",
    "\n",
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged, train)\n",
    "y_test, X_test = vec_for_learning(model_dbow, test_tagged, test)\n",
    "\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model paring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10248/10248 [00:00<00:00, 1762329.95it/s]\n",
      "100%|██████████| 10248/10248 [00:00<00:00, 3007713.06it/s]\n",
      "100%|██████████| 10248/10248 [00:00<00:00, 3113147.49it/s]\n",
      "100%|██████████| 10248/10248 [00:00<00:00, 2337696.60it/s]\n",
      "100%|██████████| 10248/10248 [00:00<00:00, 2977296.35it/s]\n",
      "100%|██████████| 10248/10248 [00:00<00:00, 3110894.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.39 s, sys: 1.5 s, total: 4.89 s\n",
      "Wall time: 3.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# If dm=0, distributed bag of words (PV-DBOW) is used; if dm=1,‘distributed memory’ (PV-DM) is used.\n",
    "# dm_mean, If 0, use the sum of the context word vectors. If 1, use the mean.\n",
    "# window, the maximum distance between the current and predicted word within a sentence\n",
    "# alpha – The initial learning rate.\n",
    "# min_alpha – Learning rate will linearly drop to min_alpha as training progresses.\n",
    "\n",
    "model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=100, window=10, negative=5, min_count=1, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "\n",
    "for epoch in range(5):\n",
    "    model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dmm.alpha -= 0.01\n",
    "    model_dmm.min_alpha = model_dmm.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "\n",
    "# Concatenate PV-DBOW and PV-DM representations \n",
    "new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qy/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/qy/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.7484061930783242\n",
      "Testing F1 score: 0.7374719680745588\n",
      "CPU times: user 26.3 s, sys: 92.2 ms, total: 26.4 s\n",
      "Wall time: 26.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def get_vectors(model, tagged_docs, data):\n",
    "    sents = tagged_docs.values\n",
    "    regressors = [model.infer_vector(doc.words, epochs=20) for doc in sents]\n",
    "    targets = data['airline_sentiment'].tolist()\n",
    "    return targets, regressors\n",
    "\n",
    "y_train, X_train = get_vectors(new_model, train_tagged, train)\n",
    "y_test, X_test = get_vectors(new_model, test_tagged, test)\n",
    "\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
