{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RGB Video data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39, 480, 640, 3)\n",
      "(480, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "from skvideo.io import vread\n",
    "\n",
    "videodata = vread(\"/Users/qy/Desktop/sense_making/data/RGB/a1_s3_t2_color.avi\")\n",
    "print(videodata.shape)\n",
    "print(videodata[0,...].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 480, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "videodata = vread(\"/Users/qy/Desktop/sense_making/data/RGB/a1_s8_t1_color.avi\")\n",
    "print(videodata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "n = 80\n",
    "step = int((n - 1) / 10)\n",
    "print(step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synchronized Depth and Skeleton data\n",
    "They have the same number of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 320, 76)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "filename = 'Depth/a1_s8_t1_depth.mat'\n",
    "mat = loadmat(filename)\n",
    "mat['d_depth'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 3, 76)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'Skeleton/a1_s8_t1_skeleton.mat'\n",
    "mat = loadmat(filename)\n",
    "mat['d_skel'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 320, 52)\n",
      "(20, 3, 52)\n"
     ]
    }
   ],
   "source": [
    "filename = 'Depth/a1_s3_t2_depth.mat'\n",
    "mat = loadmat(filename)\n",
    "print(mat['d_depth'].shape)\n",
    "\n",
    "filename = 'Skeleton/a1_s3_t2_skeleton.mat'\n",
    "mat = loadmat(filename)\n",
    "print(mat['d_skel'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 240, 64)\n",
      "3201\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "filename = 'Depth/a4_s5_t4_depth.mat'\n",
    "mat = loadmat(filename)\n",
    "print(mat['d_depth'][:,40:280,:].shape)\n",
    "print(mat['d_depth'].max())\n",
    "print(mat['d_depth'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "reference: http://media-lab.ccny.cuny.edu/wordpress/Publications/IVCJ-2016.pdf\n",
    "### Resize depth data to 240 x 240 x 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 240, 64)\n",
      "(240, 240, 64)\n"
     ]
    }
   ],
   "source": [
    "# resize a single file\n",
    "\n",
    "filename = '/Users/qy/Desktop/sense_making/data/' + f'Depth/a{action}_s{subject}_t{trial}_depth.mat'\n",
    "data = loadmat(filename)['d_depth'][:,40:280,:]\n",
    "data = np.floor((data-data.min()) / (data.max()-data.min()) * 255)\n",
    "print(data.shape)\n",
    "print(data[:,:,0:64].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (48, 240, 240, 125)\n",
      "Y_train.shape: (48,)\n",
      "X_test.shape: (16, 240, 240, 125)\n",
      "Y_test.shape: (16,)\n"
     ]
    }
   ],
   "source": [
    "# Subjects 1, 2, 3, 5, 6, 7 go into training data (75%)\n",
    "# Subjects 4, 8 go into test data (25%)\n",
    "\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from cv2 import resize\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "# for action in range(1, 28):\n",
    "for action in range(1, 3):    \n",
    "    # print(action)\n",
    "    for subject in range(1, 9):\n",
    "        for trial in range(1, 5):\n",
    "            # data = import_inertial_data(action, subject, trial)\n",
    "            filename = '/Users/qy/Desktop/sense_making/data/' + f'Depth/a{action}_s{subject}_t{trial}_depth.mat'\n",
    "            try:\n",
    "                data = loadmat(filename)['d_depth'][:,40:280,:]\n",
    "                data = np.floor((data-data.min()) / (data.max()-data.min()) * 255)\n",
    "                # resize                \n",
    "                data_stack = np.zeros((240, 240, 125))\n",
    "                n1 = data_stack.shape[2] - data.shape[2]\n",
    "                data_stack[:,:,n1:125] = data \n",
    "                \n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            # if data is None: continue\n",
    "            # data = np.swapaxes(data, 0, 1)\n",
    "            # data = sequence.pad_sequences(data, maxlen=125)\n",
    "            if subject in [1, 2 ,3, 5, 6, 7] :\n",
    "                X_train.append(data_stack)\n",
    "                Y_train.append(action-1)\n",
    "            else:\n",
    "                X_test.append(data_stack)\n",
    "                Y_test.append(action-1)\n",
    "                \n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "print('X_train.shape:', X_train.shape)\n",
    "print('Y_train.shape:', Y_train.shape)\n",
    "print('X_test.shape:', X_test.shape)\n",
    "print('Y_test.shape:', Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optical flow from OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:    \n",
    "Implementing Lucas-Kanade Optical Flow algorithm in Python    \n",
    "https://sandipanweb.wordpress.com/2018/02/25/implementing-lucas-kanade-optical-flow-algorithm-in-python/\n",
    "\n",
    "https://docs.opencv.org/3.3.1/d7/d8b/tutorial_py_lucas_kanade.html\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/jagracar/OpenCV-python-tests/blob/master/OpenCV-tutorials/videoAnalysis/denseOpticalFlow.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RGB video data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640)\n",
      "(480, 640, 2)\n",
      "(480, 640, 3)\n",
      "(480, 640)\n",
      "(480, 640, 2)\n",
      "(480, 640, 3)\n",
      "(480, 640)\n",
      "(480, 640, 2)\n",
      "(480, 640, 3)\n",
      "(480, 640)\n",
      "(480, 640, 2)\n",
      "(480, 640, 3)\n",
      "(480, 640)\n",
      "(480, 640, 2)\n",
      "(480, 640, 3)\n",
      "(5, 480, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Start the webcam\n",
    "# cap = cv2.VideoCapture(0)\n",
    "cap = cv2.VideoCapture(\"/Users/qy/Desktop/sense_making/data/RGB/a1_s3_t2_color.avi\")\n",
    "\n",
    "# Take the first frame and convert it to gray\n",
    "ret, frame = cap.read()\n",
    "gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create the HSV color image\n",
    "hsvImg = np.zeros_like(frame)\n",
    "hsvImg[..., 1] = 255\n",
    "\n",
    "optflow = []\n",
    "\n",
    "# Play until the user decides to stop\n",
    "\n",
    "for i in range(5):\n",
    "# Save the previous frame data\n",
    "    previousGray = gray\n",
    "     \n",
    "    # Get the next frame\n",
    "    ret , frame = cap.read()\n",
    "    # print(ret)\n",
    "    if ret:\n",
    "        # Convert the frame to gray scale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        print(gray.shape)\n",
    "        \n",
    "        # Calculate the dense optical flow\n",
    "        flow = cv2.calcOpticalFlowFarneback(previousGray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "        print(flow.shape)\n",
    "        \n",
    "        # Obtain the flow magnitude and direction angle\n",
    "        mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "        \n",
    "        # Update the color image\n",
    "        hsvImg[..., 0] = 0.5 * ang * 180 / np.pi\n",
    "        hsvImg[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "        rgbImg = cv2.cvtColor(hsvImg, cv2.COLOR_HSV2BGR)\n",
    "        print(rgbImg.shape)\n",
    "        optflow.append(rgbImg)\n",
    "        # print(optflow)\n",
    "        \n",
    "        # Display the resulting frame\n",
    "        # cv2.imshow('dense optical flow', np.hstack((frame, rgbImg)))\n",
    "        # k = cv2.waitKey(30) & 0xff\n",
    "        \n",
    "        # Exit if the user press ESC\n",
    "        # if k == 27:\n",
    "        #    break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "optflow = np.array(optflow)\n",
    "print(optflow.shape)\n",
    "\n",
    "# When everything is done, release the capture and close all windows\n",
    "cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 7\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "(20, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "filename = 'Depth/a1_s8_t1_depth.mat'\n",
    "mat = loadmat(filename)\n",
    "nf = mat['d_depth'].shape[2]\n",
    "step = int((nf - 1) / 10)\n",
    "print(nf, step)\n",
    "\n",
    "gray = mat['d_depth'][:,40:280,0]\n",
    "gray = cv2.resize(gray, (64, 64))\n",
    "\n",
    "optflow = []\n",
    "\n",
    "for i in range(1, 11):\n",
    "# Save the previous frame data\n",
    "    print(i)\n",
    "    previousGray = gray\n",
    "     \n",
    "    # Get the next frame\n",
    "    data = mat['d_depth'][:,40:280,i * step]\n",
    "    data = cv2.resize(data, (64, 64))\n",
    "\n",
    "    # Convert the frame to gray scale\n",
    "    gray = np.floor((data-data.min()) / (data.max()-data.min()) * 255)\n",
    "\n",
    "    # Calculate the dense optical flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(previousGray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "    # Obtain the flow magnitude and direction angle\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "\n",
    "    # Update the color image\n",
    "    Img1 = 0.5 * ang * 180 / np.pi\n",
    "    Img2 = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    # rgbImg = cv2.cvtColor(hsvImg, cv2.COLOR_HSV2BGR)\n",
    "    # print(rgbImg.shape)\n",
    "    optflow.append(Img1)\n",
    "    optflow.append(Img2)\n",
    "\n",
    "optflow = np.array(optflow)\n",
    "print(optflow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 20)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.swapaxes(np.swapaxes(optflow,0,1),1,2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Optical Flow Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optflow_depth(mat):\n",
    "\n",
    "    # mat = loadmat(filename)\n",
    "    nf = mat['d_depth'].shape[2]\n",
    "    step = int((nf - 1) / 10)\n",
    "    # print(nf, step)\n",
    "\n",
    "    gray = mat['d_depth'][:,40:280,0]\n",
    "    gray = cv2.resize(gray, (64, 64))\n",
    "    gray = np.floor((gray-gray.min()) / (gray.max()-gray.min()) * 255)\n",
    "\n",
    "    optflow = []\n",
    "\n",
    "    for i in range(1, 11):\n",
    "    # Save the previous frame data\n",
    "        previousGray = gray\n",
    "\n",
    "        # Get the next frame\n",
    "        data = mat['d_depth'][:,40:280,i * step]\n",
    "        data = cv2.resize(data, (64, 64))\n",
    "\n",
    "        # Convert the frame to gray scale\n",
    "        gray = np.floor((data-data.min()) / (data.max()-data.min()) * 255)\n",
    "\n",
    "        # Calculate the dense optical flow\n",
    "        flow = cv2.calcOpticalFlowFarneback(previousGray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "        # Obtain the flow magnitude and direction angle\n",
    "        mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "\n",
    "        # Update the color image\n",
    "        Img1 = 0.5 * ang * 180 / np.pi\n",
    "        Img2 = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "        optflow.append(Img1)\n",
    "        optflow.append(Img2)\n",
    "\n",
    "    optflow = np.array(optflow)\n",
    "    out = np.swapaxes(np.swapaxes(optflow,0,1),1,2)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Optical Flow RGB Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skvideo.io import vread\n",
    "\n",
    "# videodata = vread(\"/Users/qy/Desktop/sense_making/data/RGB/a1_s3_t2_color.avi\")\n",
    "# print(videodata.shape)\n",
    "\n",
    "def optflow_rgb(file):\n",
    "\n",
    "    mat = vread(file)\n",
    "    nf = mat.shape[0]\n",
    "    step = int((nf - 1) / 10)\n",
    "    # print(nf, step)\n",
    "\n",
    "    frame = mat[0,:,80:560,:]\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.resize(gray, (64, 64))\n",
    "    # gray = np.floor((gray-gray.min()) / (gray.max()-gray.min()) * 255)\n",
    "\n",
    "    optflow = []\n",
    "\n",
    "    for i in range(1, 11):\n",
    "    # Save the previous frame data\n",
    "        previousGray = gray\n",
    "\n",
    "        # Get the next frame\n",
    "        frame = mat[i * step,:,80:560,:]\n",
    "        # Convert the frame to gray scale\n",
    "        data = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.resize(data, (64, 64))\n",
    "\n",
    "        # gray = np.floor((data-data.min()) / (data.max()-data.min()) * 255)\n",
    "\n",
    "        # Calculate the dense optical flow\n",
    "        flow = cv2.calcOpticalFlowFarneback(previousGray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "        # Obtain the flow magnitude and direction angle\n",
    "        mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "\n",
    "        # Update the color image\n",
    "        Img1 = 0.5 * ang * 180 / np.pi\n",
    "        Img2 = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "        optflow.append(Img1)\n",
    "        optflow.append(Img2)\n",
    "\n",
    "    optflow = np.array(optflow)\n",
    "    out = np.swapaxes(np.swapaxes(optflow,0,1),1,2)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a10_s1_t1_depth.mat',\n",
       " 'a10_s1_t2_depth.mat',\n",
       " 'a10_s1_t3_depth.mat',\n",
       " 'a10_s1_t4_depth.mat',\n",
       " 'a10_s2_t1_depth.mat']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "flist = os.listdir(\"Depth/\")\n",
    "flist[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a10_s1_t1_color.avi',\n",
       " 'a10_s1_t2_color.avi',\n",
       " 'a10_s1_t3_color.avi',\n",
       " 'a10_s1_t4_color.avi',\n",
       " 'a10_s2_t1_color.avi']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flist = os.listdir(\"RGB/\")\n",
    "flist[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(64, 64, 20)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'Depth/' + 'a10_s2_t1_depth.mat'\n",
    "test = optflow_depth(file)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 20)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'RGB/' + 'a10_s1_t1_color.avi'\n",
    "test = optflow_rgb(file)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "861"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "X_train.shape: (431, 64, 64, 20)\n",
      "Y_train.shape: (431,)\n",
      "X_val.shape: (215, 64, 64, 20)\n",
      "Y_val.shape: (215,)\n",
      "X_test.shape: (215, 64, 64, 20)\n",
      "Y_test.shape: (215,)\n"
     ]
    }
   ],
   "source": [
    "# Subjects 1, 2, 3, 5, go into training data (50%)\n",
    "# Subjects 6, 7 validation\n",
    "# Subjects 4, 8 go into test data (25%)\n",
    "\n",
    "import numpy as np\n",
    "from cv2 import resize\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "X_val = []\n",
    "Y_val = []\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "for action in range(1, 28):\n",
    "# for action in range(1, 3):    \n",
    "    print(action)\n",
    "    for subject in range(1, 9):\n",
    "        for trial in range(1, 5):\n",
    "            # data = import_inertial_data(action, subject, trial)\n",
    "            filename = '/Users/qy/Desktop/sense_making/data/' + f'Depth/a{action}_s{subject}_t{trial}_depth.mat'\n",
    "            try:\n",
    "                mat = loadmat(filename)\n",
    "                data = optflow_depth(mat)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            \n",
    "            # if data is None: continue\n",
    "            # data = np.swapaxes(data, 0, 1)\n",
    "            # data = sequence.pad_sequences(data, maxlen=125)\n",
    "            if subject in [1, 2 ,3, 5] :\n",
    "                X_train.append(data)\n",
    "                Y_train.append(action-1)\n",
    "            elif subject in [6, 7]:\n",
    "                X_val.append(data)\n",
    "                Y_val.append(action-1)\n",
    "            else:\n",
    "                X_test.append(data)\n",
    "                Y_test.append(action-1)\n",
    "                \n",
    "                \n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "X_val = np.array(X_val)\n",
    "Y_val = np.array(Y_val)\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "print('X_train.shape:', X_train.shape)\n",
    "print('Y_train.shape:', Y_train.shape)\n",
    "print('X_val.shape:', X_val.shape)\n",
    "print('Y_val.shape:', Y_val.shape)\n",
    "print('X_test.shape:', X_test.shape)\n",
    "print('Y_test.shape:', Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train.shape: (431, 27)\n",
      "Y_val.shape: (215, 27)\n",
      "Y_test.shape: (215, 27)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "Y_train = to_categorical(Y_train)\n",
    "Y_test = to_categorical(Y_test)\n",
    "Y_val = to_categorical(Y_val)\n",
    "\n",
    "print('Y_train.shape:', Y_train.shape)\n",
    "print('Y_val.shape:', Y_val.shape)\n",
    "print('Y_test.shape:', Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN model for Depth data\n",
    "Ref: https://www.researchgate.net/publication/326968465_Human_Action_Recognition_based_on_3D_Convolution_Neural_Networks_from_RGBD_Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "NormalizeInput (BatchNormali (None, 64, 64, 20)        80        \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 64, 64, 64)        11584     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2048)              8390656   \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 27)                55323     \n",
      "=================================================================\n",
      "Total params: 9,416,747\n",
      "Trainable params: 9,416,707\n",
      "Non-trainable params: 40\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization, Conv2D, Dropout, Flatten, Dense\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "simple_cnn = Sequential()\n",
    "simple_cnn.add(BatchNormalization(input_shape = (64, 64, 20), \n",
    "                                  name = 'NormalizeInput'))\n",
    "\n",
    "simple_cnn.add(Conv2D(64, kernel_size = (3,3), activation='relu', padding='same'))\n",
    "simple_cnn.add(MaxPooling2D((2, 2) , strides = 2))\n",
    "\n",
    "simple_cnn.add(Conv2D(128, kernel_size = (3,3), activation='relu', padding='same'))\n",
    "simple_cnn.add(MaxPooling2D((2, 2) , strides = 2))\n",
    "\n",
    "simple_cnn.add(Conv2D(256, kernel_size = (3,3), activation='relu', padding='same'))\n",
    "simple_cnn.add(MaxPooling2D((2, 2) , strides = 2))\n",
    "\n",
    "simple_cnn.add(Conv2D(256, kernel_size = (3,3), activation='relu', padding='same'))\n",
    "simple_cnn.add(MaxPooling2D((2, 2) , strides = 2))\n",
    "\n",
    "simple_cnn.add(Flatten())\n",
    "simple_cnn.add(Dense(2048, activation='relu'))\n",
    "simple_cnn.add(Dense(27, activation='softmax'))\n",
    "\n",
    "simple_cnn.compile(optimizer = 'adam', loss = 'categorical_crossentropy', \n",
    "                   # metrics = [dice_coef, 'acc', 'mse']\n",
    "                   metrics = ['acc'])\n",
    "simple_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 431 samples, validate on 215 samples\n",
      "Epoch 1/20\n",
      " - 13s - loss: 3.9561 - acc: 0.0371 - val_loss: 3.3061 - val_acc: 0.0372\n",
      "Epoch 2/20\n",
      " - 10s - loss: 3.3000 - acc: 0.0441 - val_loss: 3.2286 - val_acc: 0.1767\n",
      "Epoch 3/20\n",
      " - 11s - loss: 3.1380 - acc: 0.1833 - val_loss: 2.7810 - val_acc: 0.1488\n",
      "Epoch 4/20\n",
      " - 10s - loss: 2.4614 - acc: 0.2691 - val_loss: 2.4701 - val_acc: 0.2651\n",
      "Epoch 5/20\n",
      " - 10s - loss: 2.1836 - acc: 0.4153 - val_loss: 2.0175 - val_acc: 0.3767\n",
      "Epoch 6/20\n",
      " - 11s - loss: 1.5487 - acc: 0.5197 - val_loss: 1.7405 - val_acc: 0.4000\n",
      "Epoch 7/20\n",
      " - 11s - loss: 1.0415 - acc: 0.6357 - val_loss: 1.9806 - val_acc: 0.4372\n",
      "Epoch 8/20\n",
      " - 11s - loss: 1.0614 - acc: 0.6404 - val_loss: 1.8172 - val_acc: 0.4791\n",
      "Epoch 9/20\n",
      " - 11s - loss: 0.8539 - acc: 0.6868 - val_loss: 1.3620 - val_acc: 0.5349\n",
      "Epoch 10/20\n",
      " - 11s - loss: 0.6974 - acc: 0.7564 - val_loss: 1.4158 - val_acc: 0.5628\n",
      "Epoch 11/20\n",
      " - 11s - loss: 0.5114 - acc: 0.8353 - val_loss: 1.5340 - val_acc: 0.4651\n",
      "Epoch 12/20\n",
      " - 12s - loss: 0.4359 - acc: 0.8376 - val_loss: 1.5913 - val_acc: 0.5535\n",
      "Epoch 13/20\n",
      " - 11s - loss: 0.3220 - acc: 0.8770 - val_loss: 1.5610 - val_acc: 0.6047\n",
      "Epoch 14/20\n",
      " - 11s - loss: 0.2739 - acc: 0.9002 - val_loss: 1.4743 - val_acc: 0.6279\n",
      "Epoch 15/20\n",
      " - 10s - loss: 0.2091 - acc: 0.9397 - val_loss: 1.5923 - val_acc: 0.6047\n",
      "Epoch 16/20\n",
      " - 10s - loss: 0.1530 - acc: 0.9513 - val_loss: 1.8284 - val_acc: 0.5860\n",
      "Epoch 17/20\n",
      " - 11s - loss: 0.1450 - acc: 0.9420 - val_loss: 1.9611 - val_acc: 0.5721\n",
      "Epoch 18/20\n",
      " - 10s - loss: 0.0720 - acc: 0.9814 - val_loss: 1.9217 - val_acc: 0.5907\n",
      "Epoch 19/20\n",
      " - 11s - loss: 0.0441 - acc: 0.9907 - val_loss: 2.0533 - val_acc: 0.5814\n",
      "Epoch 20/20\n",
      " - 11s - loss: 0.0337 - acc: 0.9954 - val_loss: 2.2742 - val_acc: 0.6047\n"
     ]
    }
   ],
   "source": [
    "history = simple_cnn.fit(X_train, Y_train, validation_data=(X_val, Y_val), \n",
    "                   epochs=20, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAEWCAYAAAB/mA49AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epoch = list(range(1,21))\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(epoch, history.history['acc'], color='b')\n",
    "plt.plot(epoch, history.history['val_acc'], color='r')\n",
    "plt.title('Depth CNN - Activity Classification')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(('Train', 'Validation'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video data preparation and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "X_train.shape: (431, 64, 64, 20)\n",
      "Y_train.shape: (431, 27)\n",
      "X_val.shape: (215, 64, 64, 20)\n",
      "Y_val.shape: (215, 27)\n",
      "X_test.shape: (215, 64, 64, 20)\n",
      "Y_test.shape: (215, 27)\n"
     ]
    }
   ],
   "source": [
    "# Subjects 1, 2, 3, 5, go into training data (50%)\n",
    "# Subjects 6, 7 validation\n",
    "# Subjects 4, 8 go into test data (25%)\n",
    "\n",
    "import numpy as np\n",
    "from cv2 import resize\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "X_val = []\n",
    "Y_val = []\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "for action in range(1, 28):\n",
    "# for action in range(1, 3):    \n",
    "    print(action)\n",
    "    for subject in range(1, 9):\n",
    "        for trial in range(1, 5):\n",
    "            # data = import_inertial_data(action, subject, trial)\n",
    "            # filename = '/Users/qy/Desktop/sense_making/data/' + f'Depth/a{action}_s{subject}_t{trial}_depth.mat'\n",
    "            filename = '/Users/qy/Desktop/sense_making/data/' + f'RGB/a{action}_s{subject}_t{trial}_color.avi'\n",
    "            try:\n",
    "                # mat = loadmat(filename)\n",
    "                data = optflow_rgb(filename)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            \n",
    "            # if data is None: continue\n",
    "            # data = np.swapaxes(data, 0, 1)\n",
    "            # data = sequence.pad_sequences(data, maxlen=125)\n",
    "            if subject in [1, 2 ,3, 5] :\n",
    "                X_train.append(data)\n",
    "                Y_train.append(action-1)\n",
    "            elif subject in [6, 7]:\n",
    "                X_val.append(data)\n",
    "                Y_val.append(action-1)\n",
    "            else:\n",
    "                X_test.append(data)\n",
    "                Y_test.append(action-1)\n",
    "                \n",
    "                \n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "X_val = np.array(X_val)\n",
    "Y_val = np.array(Y_val)\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "Y_train = to_categorical(Y_train)\n",
    "Y_test = to_categorical(Y_test)\n",
    "Y_val = to_categorical(Y_val)\n",
    "\n",
    "print('X_train.shape:', X_train.shape)\n",
    "print('Y_train.shape:', Y_train.shape)\n",
    "print('X_val.shape:', X_val.shape)\n",
    "print('Y_val.shape:', Y_val.shape)\n",
    "print('X_test.shape:', X_test.shape)\n",
    "print('Y_test.shape:', Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 431 samples, validate on 215 samples\n",
      "Epoch 1/20\n",
      " - 12s - loss: 3.7213 - acc: 0.3968 - val_loss: 2.9730 - val_acc: 0.3209\n",
      "Epoch 2/20\n",
      " - 10s - loss: 1.7203 - acc: 0.5128 - val_loss: 2.0187 - val_acc: 0.4326\n",
      "Epoch 3/20\n",
      " - 10s - loss: 1.3082 - acc: 0.6566 - val_loss: 1.6750 - val_acc: 0.4558\n",
      "Epoch 4/20\n",
      " - 11s - loss: 0.7672 - acc: 0.7657 - val_loss: 1.9595 - val_acc: 0.4512\n",
      "Epoch 5/20\n",
      " - 10s - loss: 0.6096 - acc: 0.7935 - val_loss: 2.0560 - val_acc: 0.4698\n",
      "Epoch 6/20\n",
      " - 12s - loss: 0.5154 - acc: 0.8306 - val_loss: 1.9425 - val_acc: 0.5535\n",
      "Epoch 7/20\n",
      " - 13s - loss: 0.3143 - acc: 0.8863 - val_loss: 1.6844 - val_acc: 0.5395\n",
      "Epoch 8/20\n",
      " - 12s - loss: 0.2145 - acc: 0.9281 - val_loss: 1.8605 - val_acc: 0.5256\n",
      "Epoch 9/20\n",
      " - 11s - loss: 0.1350 - acc: 0.9768 - val_loss: 1.8739 - val_acc: 0.5488\n",
      "Epoch 10/20\n",
      " - 12s - loss: 0.0686 - acc: 0.9907 - val_loss: 1.9859 - val_acc: 0.5488\n",
      "Epoch 11/20\n",
      " - 12s - loss: 0.0364 - acc: 0.9977 - val_loss: 2.2136 - val_acc: 0.5395\n",
      "Epoch 12/20\n",
      " - 11s - loss: 0.0206 - acc: 0.9977 - val_loss: 2.2958 - val_acc: 0.5581\n",
      "Epoch 13/20\n",
      " - 11s - loss: 0.0207 - acc: 0.9954 - val_loss: 2.5176 - val_acc: 0.5581\n",
      "Epoch 14/20\n",
      " - 12s - loss: 0.0100 - acc: 0.9977 - val_loss: 2.5791 - val_acc: 0.5860\n",
      "Epoch 15/20\n",
      " - 12s - loss: 0.0054 - acc: 1.0000 - val_loss: 2.5808 - val_acc: 0.6000\n",
      "Epoch 16/20\n",
      " - 11s - loss: 0.0035 - acc: 1.0000 - val_loss: 2.5740 - val_acc: 0.5953\n",
      "Epoch 17/20\n",
      " - 11s - loss: 0.0020 - acc: 1.0000 - val_loss: 2.6597 - val_acc: 0.6000\n",
      "Epoch 18/20\n",
      " - 11s - loss: 5.7632e-04 - acc: 1.0000 - val_loss: 2.6926 - val_acc: 0.6000\n",
      "Epoch 19/20\n",
      " - 11s - loss: 5.2637e-04 - acc: 1.0000 - val_loss: 2.7076 - val_acc: 0.6000\n",
      "Epoch 20/20\n",
      " - 10s - loss: 4.0831e-04 - acc: 1.0000 - val_loss: 2.7211 - val_acc: 0.6000\n"
     ]
    }
   ],
   "source": [
    "# video cnn model, same as depth model\n",
    "history = simple_cnn.fit(X_train, Y_train, validation_data=(X_val, Y_val), \n",
    "                   epochs=20, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth optical flow data\n",
    "np.savez_compressed('depth_of', X_train, Y_train, X_val, Y_val, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arr_0', 'arr_1', 'arr_2', 'arr_3', 'arr_4', 'arr_5']\n",
      "(431, 64, 64, 20)\n"
     ]
    }
   ],
   "source": [
    "loaded = np.load('depth_of.npz')\n",
    "print(loaded.files)\n",
    "print(loaded['arr_0'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video optical flow data\n",
    "np.savez_compressed('video_of', X_train, Y_train, X_val, Y_val, X_test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
